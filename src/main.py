# src/main.py
import asyncio
import pandas as pd
from datetime import datetime
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import THREADS_USERNAME, START_DATE, END_DATE, SKIP_PINNED, OUTPUT_DIR
from scraper import ThreadsScraper
from analyzer import GuidelineAnalyzer, generate_summary


async def main():
    print("=" * 70)
    print("Threads ê²Œì‹œë¬¼ ê°€ì´ë“œë¼ì¸ ë¶„ì„ê¸° (Meta ê³µì‹ ì»¤ë®¤ë‹ˆí‹° ê·œì • ê¸°ë°˜)")
    print("=" * 70)
    print(f"ì‚¬ìš©ì: @{THREADS_USERNAME}")
    print(f"ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    print(f"ìƒìœ„ ê³ ì •ê¸€ ì œì™¸: {SKIP_PINNED}ê°œ")
    print("=" * 70)
    
    # 1. í¬ë¡¤ë§ (ê³ ì •ê¸€ ì œì™¸)
    scraper = ThreadsScraper(
        username=THREADS_USERNAME,
        start_date=START_DATE,
        end_date=END_DATE,
        skip_pinned=SKIP_PINNED
    )
    posts = await scraper.scrape_posts()
    
    if not posts:
        print("[!] ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        with open(os.path.join(OUTPUT_DIR, "summary.txt"), "w", encoding="utf-8") as f:
            f.write("ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.\n")
        return
    
    # 2. ë¶„ì„
    print(f"\n[*] {len(posts)}ê°œ ê²Œì‹œë¬¼ì„ Meta ì»¤ë®¤ë‹ˆí‹° ê·œì • ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„ ì¤‘...")
    analyzer = GuidelineAnalyzer()
    results = analyzer.analyze_all_posts(posts)
    
    # 3. ê²°ê³¼ ìš”ì•½
    summary = generate_summary(results)
    
    print("\n" + "=" * 70)
    print("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    print(f"ì´ ê²Œì‹œë¬¼: {summary['total_posts']}ê°œ")
    print(f"ğŸ”´ ë§¤ìš° ë†’ìŒ (ì‚­ì œ ê°€ëŠ¥ì„±): {summary['critical_count']}ê°œ")
    print(f"ğŸŸ  ë†’ìŒ (ê²½ê³ /ì œí•œ ê°€ëŠ¥ì„±): {summary['high_risk_count']}ê°œ")
    print(f"ğŸŸ¡ ì¤‘ê°„ (ì£¼ì˜ í•„ìš”): {summary['medium_risk_count']}ê°œ")
    print(f"ğŸŸ¢ ë‚®ìŒ: {summary['low_risk_count']}ê°œ")
    print(f"âœ… ì•ˆì „: {summary['safe_count']}ê°œ")
    print(f"ë°˜ë³µ/ì¤‘ë³µ: {summary['duplicate_count']}ê°œ")
    print(f"í‰ê·  ìœ„í—˜ ì ìˆ˜: {summary['average_risk_score']}/100")
    
    if summary['top_violations']:
        print("\n[ì£¼ìš” ìœ„ë°˜ ìœ í˜•]")
        for violation, count in summary['top_violations']:
            print(f"  â€¢ {violation}: {count}ê±´")
    
    # 4. CSV ì €ì¥
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    df_data = []
    for r in results:
        violation_summary = "; ".join([
            f"{v['category']}/{v['subcategory']}" 
            for v in r.get("violations", [])
        ])
        policy_refs = "; ".join(r.get("official_policy_refs", []))
        
        df_data.append({
            "ì‚¬ìš©ìëª…": r["username"],
            "ë‚ ì§œì‹œê°„": r["datetime"],
            "ê²Œì‹œë¬¼ë‚´ìš©": r["text"],
            "ë§í¬": r["link"],
            "ì¢‹ì•„ìš”": r["likes"],
            "ë‹µê¸€": r["replies"],
            "ìœ„í—˜ì ìˆ˜": r["risk_score"],
            "ìœ„í—˜ë“±ê¸‰": r["risk_level"],
            "ìœ„ë°˜í•­ëª©": violation_summary,
            "ê´€ë ¨ì •ì±…": policy_refs,
            "ì¤‘ë³µì—¬ë¶€": "ì˜ˆ" if r.get("is_duplicate", False) else "ì•„ë‹ˆì˜¤",
            "ê¶Œê³ ì‚¬í•­": " | ".join(r.get("recommendations", []))
        })
    
    df = pd.DataFrame(df_data)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"threads_{THREADS_USERNAME}_{START_DATE}_to_{END_DATE}_{timestamp}"
    
    csv_path = os.path.join(OUTPUT_DIR, f"{filename}.csv")
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"\nâœ… CSV ì €ì¥: {csv_path}")
    
    # 5. ìš”ì•½ íŒŒì¼ ì €ì¥
    summary_path = os.path.join(OUTPUT_DIR, "summary.txt")
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(f"# Threads ê²Œì‹œë¬¼ ê°€ì´ë“œë¼ì¸ ë¶„ì„ ê²°ê³¼\n\n")
        f.write(f"### ìš”ì•½\n")
        f.write(f"- ì´ ê²Œì‹œë¬¼: **{summary['total_posts']}ê°œ**\n")
        f.write(f"- ğŸ”´ ë§¤ìš° ë†’ìŒ: **{summary['critical_count']}ê°œ**\n")
        f.write(f"- ğŸŸ  ë†’ìŒ: **{summary['high_risk_count']}ê°œ**\n")
        f.write(f"- ğŸŸ¡ ì¤‘ê°„: **{summary['medium_risk_count']}ê°œ**\n")
        f.write(f"- ğŸŸ¢ ë‚®ìŒ: **{summary['low_risk_count']}ê°œ**\n")
        f.write(f"- âœ… ì•ˆì „: **{summary['safe_count']}ê°œ**\n")
        f.write(f"- ë°˜ë³µ/ì¤‘ë³µ: **{summary['duplicate_count']}ê°œ**\n")
        f.write(f"- í‰ê·  ìœ„í—˜ ì ìˆ˜: **{summary['average_risk_score']}/100**\n\n")
        
        if summary['top_violations']:
            f.write(f"### ì£¼ìš” ìœ„ë°˜ ìœ í˜•\n")
            for violation, count in summary['top_violations']:
                f.write(f"- {violation}: {count}ê±´\n")
            f.write("\n")
        
        critical_posts = sorted(results, key=lambda x: x["risk_score"], reverse=True)[:10]
        if critical_posts and critical_posts[0]["risk_score"] > 0:
            f.write(f"### âš ï¸ ì£¼ì˜ í•„ìš” ê²Œì‹œë¬¼ (ìƒìœ„ 10ê°œ)\n\n")
            for i, post in enumerate(critical_posts, 1):
                if post["risk_score"] > 0:
                    text_preview = post["text"][:80].replace("\n", " ") + "..."
                    f.write(f"**{i}. {post['risk_level']}** (ì ìˆ˜: {post['risk_score']})\n")
                    f.write(f"- ë‚´ìš©: {text_preview}\n")
                    f.write(f"- ë‚ ì§œ: {post['datetime'][:10] if post['datetime'] else 'N/A'}\n")
                    if post.get("recommendations"):
                        f.write(f"- ê¶Œê³ : {post['recommendations'][0][:80]}...\n")
                    f.write("\n")
    
    print(f"âœ… ìš”ì•½ ì €ì¥: {summary_path}")
    print("\në¶„ì„ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(main())
